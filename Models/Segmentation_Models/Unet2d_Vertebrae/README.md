# Semantic Segmentation of Vertebra using Convolutional Neural Networks 
--------------------------------------------------------------------------

The Trained Models are provided here.

Prerequisite:

- python 3.6.1
- numpy 1.18.1
- matplotlib 3.1.2
- nibabel 3.0.0
- tensorflow 2.1.0
- keras 2.3.1

### 2. Experiments 

The dataset used in this work was extracted from the <a href="https://github.com/jsaenzBimcv/MIDAS-Project/tree/main/Datasets">MIDAS corpus</a>.
The used MR images come from scanning sessions corresponding to 100 different patients, each scanning session has a different number of slices.
The split into three partitions training, validation and test was done at the level of patient in order to guarantee no 2-D images from the same patient appear in different partitions.

The [Table 1](#table1) shows the variations from the U-Net and combination of the configuration parameters that obtained the best results for each network aechitecture.

<div align="center"> 
<sub> 

| ID   | Configuration                      | Optimiser | Lr          | Act-Conv |
|------|------------------------------------|-----------|-------------|----------|
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/UDD2.svg">UDD2</a>| U-Net + DS + DS(v2)                | Adam      | 0.00033 | ReLU     |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/UMDD.svg">UMDD</a>| U-Net + multiKernel + DS + DS      | Adam      | 0.00033 | ReLU     |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/UDD.svg">UDD</a>| U-Net + DS + DS                    | Adam      | 0.00033 | ReLU     |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/UQD.svg">UQD</a>| U-Net + DenseBlock + DS            | Adam      | 0.00033 | ReLU     |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/UVDD.svg">UVDD</a>| U-Net + VGG16 + DS + DS            | Adam      | 0.00033 | PReLU    |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/UVMD.svg">UVMD</a>| U-Net + VGG16 + multiKernel + DS   | Adam      | 0.00033 | ReLU     |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/UAMD.svg">UAMD</a>| U-Net + attGate + multiKernel + DS | Adam      | 0.00033 | ReLU     |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/UMD.svg">UMD</a>| U-Net + multiKernel + DS           | Adam      | 0.00033 | ReLU     |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/UAD.svg">UAD</a>| U-Net + attGate + DS               | RMSprop   | 0.001   | ReLU     |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/UD.svg">UD</a>| U-Net + DS                         | Adam      | 0.00033 | ReLU     |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/UA.svg">UA</a>| U-Net + attGate                    | Adam      | 0.00033 | ReLU     |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/U1.svg">U1</a>| U-Net                              | Adadelta  | 1.0     | ReLU     |
|<a href="https://github.com/jsaenzBimcv/MIDAS-Project/blob/main/Models/Segmentation_Models/Unet2d_LSSpine/models/plot_models/FCN8s.svg">FCN</a>| FCN8                               | Adam      | 0.00033 | ReLU     |

</sub>
</div>
<p align="center">
<a id="table1">Table 1:</a> Parameters settings of the CNN architectures.
</p>

All variations designed from the U-Net architecture are trained for 300 epochs, In all cases the activation function in the output layer was the softmax and the loss the categorical cross entropy.

Intersection over Union (IoU) [(Long et al., 2015)](#12) was used as the metric to compare the performance of the evaluated network architectures.

Two methods were compared to binarise each target class in the test predictions. 
(i) On the one hand, using __Maximum a Posteriori Probability Estimate (MAP)__ , Each single pixel at output is assigned to the class with the highest score generated by the softmax activation function, (ii) and on the other hand, using a __naive adaptation of threshold optimisation (TH)__, a threshold per target class was tuned using the validation set of each partition by calculating the IoU value for different thresholds, the threshold used was the one which obtained the best IoU value.  See: <a href="https://github.com/jsaenzBimcv/MIDAS-Project/tree/main/Models/Segmentation_Models/Unet2d_LSSpine/utils/"> threshold_validations</a>

### 2.2. Results

The [Table 2](#table2) shows the Intersecction Over Union (IoU) per class and the average IoU. The best results for each one of the classes have been highlighted in bold.

<p align="center">
<img src="./images/networks_results.png" align="middle" width="800px">
</p>
<p align="center">
<a id="table2">Table 2:</a> Performance of the Automatic Semantic Segmentation generated by several network architectures measured in terms of the Intersection over Union (IoU) metric on 12 classes -- Background is not a target class. Predictions were binarised using Maximum a Posteriori Probability (Top) and Threshold Optimisation (Bottom).
</p>
 

If you use this code please cite:

J. J. Saenz-Gamboa, M. de la Iglesia-Vayá and J. A. Gómez, "Automatic Semantic Segmentation of Structural Elements related to the Spinal Cord in the Lumbar Region by using Convolutional Neural Networks," 2020 25th International Conference on Pattern Recognition (ICPR), 2021, pp. 5214-5221, <a href="https://doi.org/10.1109/ICPR48806.2021.9412934">doi:10.1109/ICPR48806.2021.9412934.</a>

## Rights and permissions.

 <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>., which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.

## Acknowledgments

This research was developed as part of the PhD research:
Researcher:

In collaboration with: 
* <a href="https://">Laboratorio de , València, Spain.
* <a href="http://www.cipf.es/cipf-fisabio-joint-research-unit-biomedical-imaging">Biomedical Imaging Joint Unit, Foundation for the Promotion of Health and Biomedical Research (FISABIO) and the Principe Felipe Research Center (CIPF)</a>, València, Spain.
* Models were trained on IFIC's dedicated Artificial Intelligence computing platform, <a href="https://artemisa.ific.uv.es/web/">Artemisa</a>.

